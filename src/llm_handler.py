import logging
import sys
import io
import requests
import json
import time
import re
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum
from dataclasses import dataclass

# Forcer UTF-8 console
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    elif hasattr(sys.stdout, "buffer"):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")
    elif hasattr(sys.stderr, "buffer"):
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")
except Exception:
    pass

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Types et Enums
class LLMBackend(Enum):
    OLLAMA = "ollama"

class PromptTemplate(Enum):
    STANDARD = "standard"
    CONCISE = "concise"
    DETAILED = "detailed"

@dataclass
class GenerationConfig:
    """
    Configuration g√©n√©ration LLM - Optimis√©e pour Llama3.2:3b
    """
    temperature: float = 0.3        # L√©g√®rement plus haut pour plus de cr√©ativit√©
    top_p: float = 0.9
    top_k: int = 40
    max_tokens: int = 512           # R√©duit pour √©viter les timeouts
    num_ctx: int = 4096             # Contexte r√©duit pour stabilit√©
    repeat_penalty: float = 1.1     # P√©nalit√© r√©duite
    stop_sequences: List[str] = None
    
    def __post_init__(self):
        if self.stop_sequences is None:
            self.stop_sequences = ["<|eot_id|>", "###", "Human:", "User:"]
    
    def to_ollama_dict(self) -> Dict[str, Any]:
        return {
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "num_predict": self.max_tokens,
            "num_ctx": self.num_ctx,
            "repeat_penalty": self.repeat_penalty,
            "stop": self.stop_sequences
        }

@dataclass
class LLMResponse:
    text: str
    model: str
    backend: str
    generation_time: float
    tokens_generated: int
    tokens_per_second: float
    context_used: bool
    sources: List[str]
    success: bool
    error: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'response': self.text,
            'model': self.model,
            'backend': self.backend,
            'generation_time': self.generation_time,
            'tokens_generated': self.tokens_generated,
            'tokens_per_second': self.tokens_per_second,
            'context_used': self.context_used,
            'sources': self.sources,
            'success': self.success,
            'error': self.error
        }

class OllamaHandler:
    """
    Handler LLM pour Ollama avec mod√®le Llama3.2:3b
    Version corrig√©e pour les erreurs 500
    """
    
    OLLAMA_BASE_URL = "http://localhost:11434"
    DEFAULT_MODEL = "llama3.2:3b"
    REQUEST_TIMEOUT = 60            # Timeout augment√©
    MAX_RETRIES = 2
    RETRY_DELAY = 3
    
    def __init__(
        self,
        model: str = DEFAULT_MODEL,
        generation_config: Optional[GenerationConfig] = None
    ):
        self.model = model
        self.generation_config = generation_config or GenerationConfig()
        
        # Session HTTP avec timeout plus long
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
        })
        
        # Statistiques
        self.stats = {
            'total_requests': 0,
            'errors': 0,
            'retries': 0
        }
        
        # V√©rifier que le mod√®le est disponible
        self._verify_model()
        
        logger.info(f"[SUCCESS] Ollama Handler initialis√© - Mod√®le: {self.model}")
    
    def _verify_model(self) -> None:
        """V√©rifie que le mod√®le Llama3.2 est disponible"""
        try:
            response = requests.get(
                f"{self.OLLAMA_BASE_URL}/api/tags",
                timeout=10
            )
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.model not in model_names:
                    logger.warning(f"[WARNING] Mod√®le {self.model} non trouv√©")
                    logger.warning(f"Mod√®les disponibles: {model_names}")
                    logger.info(f"üí° Pour installer: ollama pull {self.model}")
                    
                    # Sugg√©rer des alternatives
                    alternatives = ["llama3.2:3b", "llama3.2:1b", "llama3.1:8b", "llama3.2"]
                    for alt in alternatives:
                        if alt in model_names:
                            logger.info(f"üîÑ Utilisation alternative: {alt}")
                            self.model = alt
                            break
                    else:
                        # Si aucun mod√®le alternatif, utiliser le premier disponible
                        if model_names:
                            logger.info(f"üîÑ Utilisation du premier mod√®le disponible: {model_names[0]}")
                            self.model = model_names[0]
                else:
                    logger.info(f"‚úÖ Mod√®le {self.model} trouv√© et disponible")
            else:
                logger.warning("‚ö†Ô∏è Impossible de v√©rifier les mod√®les Ollama")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification mod√®le: {e}")
            logger.info("üí° V√©rifiez que Ollama est d√©marr√©: ollama serve")
    
    def _build_simple_prompt(
        self,
        question: str,
        context_docs: List[Dict[str, Any]]
    ) -> str:
        """
        Construit un prompt SIMPLE et ROBUSTE pour Llama3.2
        √âvite les formats complexes qui peuvent causer des erreurs 500
        """
        # Construire contexte de mani√®re simple
        context_parts = []
        for i, doc in enumerate(context_docs[:3]):  # Limiter √† 3 documents
            text = doc.get('text', doc.get('contenu', ''))
            # Limiter la longueur du texte
            text_excerpt = text[:500] if len(text) > 500 else text
            context_parts.append(f"Document {i+1}: {text_excerpt}")
        
        context_text = "\n\n".join(context_parts)
        
        # Prompt SIMPLE sans formatage complexe
        prompt = f"""Contexte technique sur l'agriculture au Burkina Faso:

{context_text}

Question: {question}

En tant qu'expert agricole pour le Burkina Faso, r√©ponds en fran√ßais de fa√ßon claire et pratique en te basant sur les documents ci-dessus. Sois pr√©cis et donne des conseils applicables.

R√©ponse:"""
        
        return prompt
    
    def _build_agricultural_prompt(
        self,
        question: str,
        context_docs: List[Dict[str, Any]],
        template: PromptTemplate = PromptTemplate.STANDARD
    ) -> str:
        """
        Construit un prompt optimis√© pour Llama3.2 et l'agriculture BF
        Version simplifi√©e pour √©viter les erreurs 500
        """
        # Construire section contexte
        context_parts = []
        
        for i, doc in enumerate(context_docs[:3]):  # R√©duit √† 3 documents max
            text = doc.get('text', doc.get('contenu', ''))
            metadata = doc.get('metadata', {})
            
            source = metadata.get('titre', f'Document {i+1}')
            # Limiter la longueur du texte
            text_excerpt = text[:400] if len(text) > 400 else text
            
            context_parts.append(f"[{source}]\n{text_excerpt}")
        
        context_text = "\n\n".join(context_parts)
        
        # Templates SIMPLIFI√âS pour √©viter les erreurs
        if template == PromptTemplate.CONCISE:
            prompt = f"""Contexte:
{context_text}

Question: {question}

R√©ponds de fa√ßon concise (3-5 phrases) en tant que conseiller agricole pour le Burkina Faso:"""
        
        elif template == PromptTemplate.DETAILED:
            prompt = f"""Documents de r√©f√©rence sur l'agriculture burkinab√®:
{context_text}

Question: {question}

En tant qu'expert agricole, fournis une r√©ponse d√©taill√©e et structur√©e:
1. R√©ponse principale
2. Explications techniques  
3. Conseils pratiques
4. Sources utilis√©es

R√©ponse:"""
        
        else:  # STANDARD
            prompt = f"""Base de connaissances agricoles Burkina Faso:
{context_text}

Question: {question}

En tant que conseiller agricole expert, r√©ponds en fran√ßais de fa√ßon claire et pratique. Base ta r√©ponse sur les documents ci-dessus. Sois pr√©cis sur les techniques, quantit√©s et p√©riodes.

R√©ponse:"""
        
        return prompt
    
    def generate_answer(
        self,
        question: str,
        context_docs: List[Dict[str, Any]],
        template: PromptTemplate = PromptTemplate.STANDARD,
        use_simple_prompt: bool = True  # Option pour utiliser le prompt simple
    ) -> LLMResponse:
        """
        G√©n√®re une r√©ponse avec Ollama Llama3.2
        Version robuste avec gestion d'erreurs am√©lior√©e
        """
        try:
            self.stats['total_requests'] += 1
            
            if not question or not question.strip():
                raise ValueError("Question vide")
            
            if not context_docs:
                return self._generate_fallback_response(question)
            
            # Construire prompt (simple par d√©faut pour plus de stabilit√©)
            if use_simple_prompt:
                prompt = self._build_simple_prompt(question, context_docs)
            else:
                prompt = self._build_agricultural_prompt(question, context_docs, template)
            
            logger.debug(f"Prompt length: {len(prompt)} characters")
            
            # G√©n√©rer avec Ollama
            raw_response, metadata = self._generate_with_ollama(prompt)
            
            # Post-processing
            cleaned_response, sources = self._post_process_response(raw_response, context_docs)
            
            # Construire objet r√©ponse
            llm_response = LLMResponse(
                text=cleaned_response,
                model=metadata.get('model', self.model),
                backend='ollama',
                generation_time=metadata.get('generation_time', 0),
                tokens_generated=metadata.get('tokens_generated', 0),
                tokens_per_second=metadata.get('tokens_per_second', 0),
                context_used=True,
                sources=sources,
                success=True
            )
            
            logger.info(f"‚úÖ R√©ponse g√©n√©r√©e: '{question[:40]}...'")
            
            return llm_response
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"‚ùå √âchec g√©n√©ration: {e}")
            return self._generate_error_response(question, str(e))
    
    def _generate_with_ollama(
        self,
        prompt: str,
        retry_count: int = 0
    ) -> Tuple[str, Dict[str, Any]]:
        """
        G√©n√®re r√©ponse avec Ollama et Llama3.2
        Version avec meilleure gestion d'erreurs
        """
        try:
            # Payload SIMPLIFI√â pour √©viter les erreurs
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.generation_config.temperature,
                    "top_p": self.generation_config.top_p,
                    "num_predict": self.generation_config.max_tokens,
                }
            }
            
            logger.info(f"üîÑ G√©n√©ration avec {self.model} (tentative {retry_count + 1})...")
            start_time = time.time()
            
            response = self.session.post(
                f"{self.OLLAMA_BASE_URL}/api/generate",
                json=payload,
                timeout=self.REQUEST_TIMEOUT
            )
            
            # V√©rifier le statut HTTP
            if response.status_code != 200:
                error_msg = f"Erreur HTTP {response.status_code}"
                try:
                    error_detail = response.json().get('error', 'No details')
                    error_msg += f": {error_detail}"
                except:
                    error_msg += f": {response.text}"
                
                logger.error(f"‚ùå {error_msg}")
                raise Exception(error_msg)
            
            data = response.json()
            
            generation_time = time.time() - start_time
            
            # M√©tadonn√©es
            metadata = {
                'model': data.get('model', self.model),
                'generation_time': generation_time,
                'total_duration': data.get('total_duration', 0) / 1e9,
                'tokens_generated': data.get('eval_count', 0),
                'tokens_prompt': data.get('prompt_eval_count', 0),
                'tokens_per_second': data.get('eval_count', 0) / generation_time if generation_time > 0 else 0
            }
            
            logger.info(f"‚úÖ G√©n√©ration r√©ussie: {metadata['tokens_generated']} tokens en {generation_time:.2f}s")
            
            return data.get('response', ''), metadata
            
        except requests.exceptions.Timeout:
            logger.warning(f"‚è±Ô∏è Timeout Ollama (tentative {retry_count + 1})")
            if retry_count < self.MAX_RETRIES:
                self.stats['retries'] += 1
                time.sleep(self.RETRY_DELAY)
                return self._generate_with_ollama(prompt, retry_count + 1)
            else:
                raise TimeoutError("Ollama timeout apr√®s plusieurs tentatives")
                
        except requests.exceptions.ConnectionError:
            logger.error(f"üîå Erreur connexion Ollama")
            if retry_count < self.MAX_RETRIES:
                self.stats['retries'] += 1
                time.sleep(self.RETRY_DELAY * 2)  # Attente plus longue pour connexion
                return self._generate_with_ollama(prompt, retry_count + 1)
            else:
                raise ConnectionError("Impossible de se connecter √† Ollama")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur Ollama: {e}")
            if retry_count < self.MAX_RETRIES:
                self.stats['retries'] += 1
                time.sleep(self.RETRY_DELAY)
                return self._generate_with_ollama(prompt, retry_count + 1)
            else:
                raise
    
    def _post_process_response(
        self,
        raw_response: str,
        context_docs: List[Dict]
    ) -> Tuple[str, List[str]]:
        """
        Post-traite la r√©ponse brute de Llama3.2
        """
        response = raw_response.strip()
        
        # Nettoyer la r√©ponse
        for stop_seq in ['###', '---', 'Question:', 'User:', 'Human:']:
            if stop_seq in response:
                response = response.split(stop_seq)[0].strip()
        
        # Limiter longueur excessive
        if len(response) > 2000:
            response = response[:2000] + "..."
        
        # Extraire sources mentionn√©es
        sources = []
        for doc in context_docs:
            metadata = doc.get('metadata', {})
            source_name = metadata.get('titre', '') or metadata.get('source', '')
            if source_name and source_name.lower() in response.lower():
                sources.append(source_name)
        
        # Si aucune source, utiliser les sources du contexte
        if not sources:
            sources = [
                doc.get('metadata', {}).get('titre', 'Document') 
                for doc in context_docs[:2]
            ]
        
        # Nettoyer espaces
        response = re.sub(r'\n{3,}', '\n\n', response)
        response = re.sub(r' {2,}', ' ', response)
        response = response.strip()
        
        return response, sources
    
    def _generate_fallback_response(self, question: str) -> LLMResponse:
        """G√©n√®re r√©ponse fallback quand pas de contexte"""
        fallback_text = (
            "Je n'ai pas trouv√© d'informations sp√©cifiques dans ma base de connaissances "
            "agricoles pour r√©pondre √† votre question sur le Burkina Faso. "
            "Pour des conseils pr√©cis, je vous recommande de consulter les services "
            "agricoles locaux ou les organisations sp√©cialis√©es."
        )
        
        return LLMResponse(
            text=fallback_text,
            model=self.model,
            backend='fallback',
            generation_time=0,
            tokens_generated=len(fallback_text.split()),
            tokens_per_second=0,
            context_used=False,
            sources=[],
            success=True
        )
    
    def _generate_error_response(self, question: str, error: str) -> LLMResponse:
        """G√©n√®re r√©ponse d'erreur"""
        error_text = (
            "D√©sol√©, je rencontre actuellement des difficult√©s techniques. "
            "Veuillez r√©essayer dans quelques instants."
        )
        
        return LLMResponse(
            text=error_text,
            model='error',
            backend='error',
            generation_time=0,
            tokens_generated=0,
            tokens_per_second=0,
            context_used=False,
            sources=[],
            success=False,
            error=error
        )
    
    def health_check(self) -> Dict[str, Any]:
        """V√©rifie l'√©tat d'Ollama et du mod√®le"""
        try:
            response = requests.get(
                f"{self.OLLAMA_BASE_URL}/api/tags",
                timeout=5
            )
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_available = self.model in [m['name'] for m in models]
                
                return {
                    'status': 'healthy' if model_available else 'model_missing',
                    'model_available': model_available,
                    'available_models': [m['name'] for m in models],
                    'current_model': self.model
                }
        except Exception as e:
            logger.error(f"Health check failed: {e}")
        
        return {'status': 'unavailable'}
    
    def test_simple_generation(self) -> bool:
        """
        Test simple de g√©n√©ration avec une requ√™te basique
        """
        try:
            test_prompt = "Explique l'agriculture en une phrase."
            
            payload = {
                "model": self.model,
                "prompt": test_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 50
                }
            }
            
            response = self.session.post(
                f"{self.OLLAMA_BASE_URL}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"‚úÖ Test simple r√©ussi: {data.get('response', '')[:50]}...")
                return True
            else:
                logger.error(f"‚ùå Test simple √©chou√©: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Test simple √©chou√©: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """Retourne statistiques d'utilisation"""
        error_rate = (
            self.stats['errors'] / self.stats['total_requests'] * 100 
            if self.stats['total_requests'] > 0 else 0
        )
        
        return {
            **self.stats,
            'error_rate': f"{error_rate:.1f}%",
            'current_model': self.model
        }

def test_llama_handler():
    """Test robuste du handler Llama3.2"""
    logger.info("üß™ Test Ollama Handler avec Llama3.2...")
    
    try:
        handler = OllamaHandler(model="llama3.2:3b")
        
        # Health check d√©taill√©
        health = handler.health_check()
        logger.info(f"√âtat Ollama: {health['status']}")
        logger.info(f"Mod√®le courant: {health['current_model']}")
        
        if health['status'] != 'healthy':
            logger.error("‚ùå Ollama ou le mod√®le n'est pas disponible")
            return False
        
        # Test simple de g√©n√©ration d'abord
        logger.info("üîß Test simple de g√©n√©ration...")
        simple_test = handler.test_simple_generation()
        if not simple_test:
            logger.error("‚ùå Test simple √©chou√© - v√©rifiez Ollama")
            return False
        
        # Test complet avec contexte
        logger.info("üß™ Test complet avec contexte...")
        test_docs = [{
            'text': "Le mil est une c√©r√©ale r√©sistante √† la s√©cheresse. Au Burkina Faso, il se s√®me en juin-juillet avec 100-150 kg/ha d'engrais NPK.",
            'metadata': {'titre': 'Culture du mil', 'source': 'Guide FAO'}
        }]
        
        question = "Quand semer le mil au Burkina Faso ?"
        response = handler.generate_answer(
            question, 
            test_docs,
            use_simple_prompt=True  # Utiliser le prompt simple pour le test
        )
        
        if response.success:
            logger.info("‚úÖ Test complet r√©ussi avec Llama3.2")
            logger.info(f"Question: {question}")
            logger.info(f"R√©ponse: {response.text}")
            logger.info(f"Performance: {response.tokens_per_second:.1f} tokens/sec")
            
            stats = handler.get_statistics()
            logger.info(f"Statistiques: {stats}")
            
            return True
        else:
            logger.error(f"‚ùå √âchec g√©n√©ration: {response.error}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Erreur test: {e}")
        return False

def main():
    """Script principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ollama Handler pour Llama3.2')
    parser.add_argument('--mode', choices=['test', 'interactive', 'health', 'simple-test'],
                       default='test', help='Mode op√©ration')
    parser.add_argument('--question', type=str,
                       help='Question pour mode interactive')
    parser.add_argument('--model', type=str, default="llama3.2:3b",
                       help='Mod√®le Ollama √† utiliser')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        success = test_llama_handler()
        sys.exit(0 if success else 1)
        
    elif args.mode == 'health':
        handler = OllamaHandler(model=args.model)
        health = handler.health_check()
        print(f"\nü©∫ HEALTH CHECK:")
        print(f"Statut: {health['status']}")
        print(f"Mod√®le courant: {health['current_model']}")
        print(f"Mod√®le disponible: {health['model_available']}")
        print(f"Mod√®les install√©s: {health.get('available_models', [])}")
        
    elif args.mode == 'simple-test':
        handler = OllamaHandler(model=args.model)
        success = handler.test_simple_generation()
        print(f"Test simple: {'‚úÖ R√âUSSI' if success else '‚ùå √âCHEC'}")
        
    elif args.mode == 'interactive':
        handler = OllamaHandler(model=args.model)
        
        question = args.question or "Quel engrais pour le sorgho ?"
        test_docs = [{
            'text': "Le sorgho n√©cessite 150 kg/ha d'engrais NPK 14-23-14 au semis.",
            'metadata': {'titre': 'Fertilisation sorgho', 'source': 'CIRAD'}
        }]
        
        response = handler.generate_answer(question, test_docs, use_simple_prompt=True)
        
        print(f"\n‚ùì QUESTION: {question}")
        print(f"\nü§ñ R√âPONSE (Llama3.2):")
        print("=" * 70)
        print(response.text)
        print("=" * 70)
        print(f"\nüìà M√âTRIQUES:")
        print(f"  Mod√®le: {response.model}")
        print(f"  Temps: {response.generation_time:.2f}s")
        print(f"  Tokens: {response.tokens_generated}")
        print(f"  Vitesse: {response.tokens_per_second:.1f} tokens/sec")
        print(f"  Succ√®s: {response.success}")

if __name__ == "__main__":
    main()