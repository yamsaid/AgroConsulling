"""
Vector Store Module - Production Ready
Syst√®me de stockage vectoriel pour RAG Agricole Burkina Faso

Auteur: Expert ML Team
Date: 3 Novembre 2025
Hackathon: MTDPCE 2025

Ce module fournit deux impl√©mentations:
1. ChromaVectorStore : Interface ChromaDB (recommand√© pour prototypage)
2. FAISSVectorStore : Interface FAISS (recommand√© pour production)

Caract√©ristiques:
- Support embeddings 768D (LaBSE) et 384D (MiniLM)
- Persistance sur disque
- Recherche s√©mantique optimis√©e
- Batch processing intelligent
- Gestion robuste des erreurs
- Logging professionnel
"""

import os
import sys
import io

# Forcer UTF-8 sur la console (Windows) pour √©viter UnicodeEncodeError
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    elif hasattr(sys.stdout, "buffer"):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")
    elif hasattr(sys.stderr, "buffer"):
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")
except Exception:
    pass

# D√©sactiver la t√©l√©m√©trie ChromaDB AVANT l'import pour √©viter les erreurs PostHog
os.environ['ANONYMIZED_TELEMETRY'] = 'False'

import chromadb
from chromadb.config import Settings
import faiss
import numpy as np
import pickle
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
import time

# Configuration logging professionnelle

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('vector_store.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)
logging.getLogger('chromadb.telemetry.product.posthog').setLevel(logging.CRITICAL)


@dataclass
class SearchResult:
    """Structure de donn√©es pour r√©sultats de recherche"""
    document_id: str
    document_text: str
    metadata: Dict[str, Any]
    similarity_score: float
    rank: int


class BaseVectorStore:
    """Classe de base abstraite pour vector stores"""
    
    # Dimensions support√©es pour embeddings
    SUPPORTED_DIMENSIONS = {
        'labse': 768,           # LaBSE (multilingue)
        'minilm': 384,          # MiniLM (l√©ger)
        'labse-base': 768,      # Alias
        'all-minilm': 384       # Alias
    }
    
    def __init__(self, persist_directory: str = "./data/vector_db"):
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)
        self.embedding_dimension = None
        self.document_count = 0
        
    def detect_embedding_dimension(self, sample_embedding: Union[List[float], np.ndarray]) -> int:
        """
        D√©tecte automatiquement la dimension des embeddings
        
        Args:
            sample_embedding: Un embedding de test
            
        Returns:
            int: Dimension d√©tect√©e
        """
        if isinstance(sample_embedding, np.ndarray):
            dim = sample_embedding.shape[-1]
        else:
            dim = len(sample_embedding)
        
        logger.info(f" Dimension embeddings d√©tect√©e: {dim}")
        
        # Valider dimension
        if dim not in self.SUPPORTED_DIMENSIONS.values():
            logger.warning(f"[WARNING] Dimension {dim} inhabituelle (attendu: {list(self.SUPPORTED_DIMENSIONS.values())})")
        
        self.embedding_dimension = dim
        return dim
    
    def validate_corpus_data(self, corpus: List[Dict], embeddings: Union[List, np.ndarray]) -> bool:
        """
        Validation compl√®te des donn√©es corpus + embeddings
        
        Args:
            corpus: Liste de documents du corpus
            embeddings: Embeddings correspondants
            
        Returns:
            bool: True si validation OK
            
        Raises:
            ValueError: Si validation √©choue
        """
        # V√©rifier coh√©rence longueurs
        if len(corpus) != len(embeddings):
            raise ValueError(f"Incoh√©rence: {len(corpus)} docs vs {len(embeddings)} embeddings")
        
        # V√©rifier corpus non vide
        if not corpus:
            raise ValueError("Corpus vide !")
        
        # V√©rifier structure documents
        required_fields = ['id', 'titre', 'contenu', 'source']
        for i, doc in enumerate(corpus[:5]):  # V√©rifier √©chantillon
            for field in required_fields:
                if field not in doc:
                    raise ValueError(f"Document {i} manque champ '{field}'")
            
            # V√©rifier contenu non vide
            if not doc['contenu'] or len(doc['contenu'].strip()) < 10:
                logger.warning(f"[WARNING] Document {doc['id']} a un contenu tr√®s court")
        
        # D√©tecter dimension embeddings
        self.detect_embedding_dimension(embeddings[0])
        
        # V√©rifier coh√©rence dimensions
        if isinstance(embeddings, np.ndarray):
            if embeddings.shape[1] != self.embedding_dimension:
                raise ValueError(f"Dimensions embeddings incoh√©rentes: {embeddings.shape}")
        else:
            for i, emb in enumerate(embeddings[:10]):  # V√©rifier √©chantillon
                if len(emb) != self.embedding_dimension:
                    raise ValueError(f"Embedding {i} a dimension {len(emb)} au lieu de {self.embedding_dimension}")
        
        logger.info(f"[SUCCESS] Validation r√©ussie: {len(corpus)} documents, dimension {self.embedding_dimension}")
        return True


# ============================================================================
# IMPL√âMENTATION 1 : CHROMADB (Interface am√©lior√©e de votre code)
# ============================================================================

# Registre global pour g√©rer les clients ChromaDB (√©vite les instances multiples)
_chroma_clients = {}

def _get_chroma_client(path: str, settings: Settings) -> chromadb.PersistentClient:
    """
    Obtient ou cr√©e un client ChromaDB pour un chemin donn√©.
    √âvite les erreurs "instance already exists" en r√©utilisant les clients.
    
    Args:
        path: Chemin du r√©pertoire ChromaDB
        settings: Param√®tres ChromaDB
        
    Returns:
        chromadb.PersistentClient: Instance du client
    """
    path_str = str(path)
    
    # V√©rifier si un client existe d√©j√† pour ce chemin dans notre registre
    if path_str in _chroma_clients:
        return _chroma_clients[path_str]
    
    # Cr√©er nouveau client
    try:
        client = chromadb.PersistentClient(path=path_str, settings=settings)
        _chroma_clients[path_str] = client
        return client
    except Exception as e:
        error_msg = str(e)
        if "already exists" in error_msg or "already exist" in error_msg:
            # Une instance existe d√©j√† au niveau global de ChromaDB
            # On ne peut pas la r√©cup√©rer directement, mais on peut
            # cr√©er un nouveau client avec les param√®tres minimaux qui devrait
            # fonctionner car l'instance existe d√©j√†
            logger.warning(f"[WARNING] Instance ChromaDB existante d√©tect√©e pour {path_str}, cr√©ation avec param√®tres minimaux...")
            try:
                # Essayer avec les param√®tres minimaux (sans allow_reset si pr√©sent)
                minimal_settings = Settings(
                    anonymized_telemetry=False,
                    is_persistent=True
                )
                client = chromadb.PersistentClient(path=path_str, settings=minimal_settings)
                _chroma_clients[path_str] = client
                return client
            except Exception as e2:
                # Dernier recours: cr√©er sans param√®tres sp√©cifiques
                logger.warning(f"[WARNING] Tentative avec param√®tres par d√©faut...")
                client = chromadb.PersistentClient(path=path_str)
                _chroma_clients[path_str] = client
                return client
        else:
            raise


class ChromaVectorStore(BaseVectorStore):
    """
    Vector Store bas√© sur ChromaDB - Version Optimis√©e
    
    Am√©liorations par rapport au code original:
    - Auto-d√©tection dimension embeddings
    - M√©thodes save() et load()
    - Meilleure gestion m√©moire
    - Performance optimis√©e pour 500+ documents
    - Format SearchResult standardis√©
    
    Usage:
        >>> store = ChromaVectorStore("./data/chroma_db")
        >>> store.create_index(corpus, embeddings)
        >>> results = store.search(query_embedding, k=5)
    """
    
    DEFAULT_COLLECTION_NAME = "agriculture_burkina"
    DEFAULT_BATCH_SIZE = 100
    
    def __init__(self, persist_directory: str = "./data/chroma_db"):
        super().__init__(persist_directory)
        self.client = None
        self.collection = None
        self.corpus_mapping = {}  # Mapping ID -> Document complet
    
    def create_index(self, corpus: List[Dict], embeddings: Union[List, np.ndarray],
                    collection_name: str = DEFAULT_COLLECTION_NAME,
                    reset: bool = False) -> bool:
        """
        Cr√©e l'index vectoriel √† partir du corpus
        
        Args:
            corpus: Liste de documents (format corpus.json)
            embeddings: Embeddings correspondants
            collection_name: Nom de la collection ChromaDB
            reset: Si True, supprime collection existante
            
        Returns:
            bool: True si succ√®s
        """
        try:
            logger.info("[LAUNCH] Cr√©ation index ChromaDB...")
            start_time = time.time()
            
            # Validation donn√©es
            self.validate_corpus_data(corpus, embeddings)
            
            # Initialiser ChromaDB (utiliser le registre pour √©viter les instances multiples)
            settings = Settings(
                anonymized_telemetry=False,
                allow_reset=True,
                is_persistent=True
            )
            self.client = _get_chroma_client(str(self.persist_directory), settings)
            
            # G√©rer reset si demand√©
            if reset:
                try:
                    self.client.delete_collection(collection_name)
                    logger.info(f"[DELETE] Collection '{collection_name}' supprim√©e")
                except:
                    pass
            
            # Cr√©er ou r√©cup√©rer collection
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "description": "Base de connaissances agricoles Burkina Faso",
                    "domain": "agriculture",
                    "language": "french",
                    "embedding_dimension": str(self.embedding_dimension),
                    "model": "LaBSE" if self.embedding_dimension == 768 else "MiniLM",
                    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "hackathon": "MTDPCE_2025"
                }
            )
            
            # Pr√©parer donn√©es pour ChromaDB
            documents = []
            metadatas = []
            ids = []
            embeddings_list = []
            
            for i, doc in enumerate(corpus):
                # Stocker mapping complet
                self.corpus_mapping[doc['id']] = doc
                
                # Pr√©parer pour ChromaDB
                documents.append(doc['contenu'][:1000])  # Limiter taille texte stock√©
                
                metadatas.append({
                    'id': doc['id'],
                    'titre': doc['titre'][:200],  # Limiter taille
                    'source': doc.get('source', 'Unknown')[:200],
                    'organisme': doc.get('organisme', 'Unknown'),
                    'type': doc.get('type', 'general')
                })
                
                ids.append(doc['id'])
                
                # Convertir embedding en liste Python
                if isinstance(embeddings, np.ndarray):
                    embeddings_list.append(embeddings[i].tolist())
                else:
                    embeddings_list.append(embeddings[i] if isinstance(embeddings[i], list) else embeddings[i].tolist())
            
            # Insertion par batch pour performance
            batch_size = self.DEFAULT_BATCH_SIZE
            total_batches = (len(corpus) + batch_size - 1) // batch_size
            
            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min(start_idx + batch_size, len(corpus))
                
                self.collection.add(
                    embeddings=embeddings_list[start_idx:end_idx],
                    documents=documents[start_idx:end_idx],
                    metadatas=metadatas[start_idx:end_idx],
                    ids=ids[start_idx:end_idx]
                )
                
                logger.info(f" Batch {batch_num+1}/{total_batches} ajout√© ({end_idx-start_idx} docs)")
            
            self.document_count = len(corpus)
            elapsed = time.time() - start_time
            
            logger.info(f"[SUCCESS] Index cr√©√©: {self.document_count} documents en {elapsed:.2f}s")
            # √âviter division par z√©ro si elapsed est trop petit
            if elapsed > 1e-6:  # Plus de 1 microseconde
                logger.info(f"[STATS] Vitesse: {self.document_count/elapsed:.1f} docs/sec")
            else:
                logger.info(f"[STATS] Vitesse: > {self.document_count/1e-6:.1f} docs/sec (temps < 1¬µs)")
            
            # Sauvegarder corpus mapping
            self.save()
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur cr√©ation index: {e}")
            raise
    
    def search(self, query_embedding: Union[List[float], np.ndarray], 
            k: int = 5,
            filter_metadata: Optional[Dict] = None) -> List[SearchResult]:
        """Recherche s√©mantique dans le vector store"""
        try:
            if not self.collection:
                raise RuntimeError("Index non initialis√©. Appeler create_index() d'abord.")
            
            # ‚úÖ Convertir query en format ChromaDB
            if isinstance(query_embedding, np.ndarray):
                query_embedding = query_embedding.tolist()
            
            # ‚úÖ S'assurer du format [[embedding]]
            if not isinstance(query_embedding, list):
                query_embedding = query_embedding.tolist()
            
            if len(query_embedding) > 0 and not isinstance(query_embedding[0], list):
                query_embedding = [query_embedding]
            
            # Recherche
            start_time = time.time()
            
            chroma_results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=k,
                where=filter_metadata
            )
            
            elapsed = time.time() - start_time
            
            # Formater r√©sultats
            results = []
            
            if chroma_results['documents'] and chroma_results['documents'][0]:
                for rank, (doc_id, distance) in enumerate(zip(
                    chroma_results['ids'][0],
                    chroma_results['distances'][0]
                ), 1):
                    
                    # R√©cup√©rer document complet
                    full_doc = self.corpus_mapping.get(doc_id, {})
                    
                    # ‚úÖ Convertir distance en similarit√© (ChromaDB utilise L2 apr√®s normalisation)
                    similarity = 1.0 / (1.0 + distance)
                    
                    results.append(SearchResult(
                        document_id=doc_id,
                        document_text=full_doc.get('contenu', ''),
                        metadata={
                            'titre': full_doc.get('titre', 'Unknown'),
                            'source': full_doc.get('source', 'Unknown'),
                            'organisme': full_doc.get('organisme', 'Unknown'),
                            'type': full_doc.get('type', 'general')
                        },
                        similarity_score=similarity,
                        rank=rank
                    ))
            
            logger.info(f"üîç [SEARCH] Recherche: {len(results)} r√©sultats en {elapsed*1000:.1f}ms")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå [ERROR] Erreur recherche: {e}")
            import traceback
            traceback.print_exc()
            return []


    def save(self, metadata_file: str = "corpus_mapping.pkl") -> bool:
        """
        Sauvegarde le mapping corpus (ChromaDB se sauvegarde automatiquement)
        
        Args:
            metadata_file: Nom du fichier de sauvegarde
            
        Returns:
            bool: True si succ√®s
        """
        try:
            mapping_path = self.persist_directory / metadata_file
            
            with open(mapping_path, 'wb') as f:
                pickle.dump(self.corpus_mapping, f)
            
            logger.info(f" Corpus mapping sauvegard√©: {mapping_path}")
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur sauvegarde: {e}")
            return False
    
    
    def load(self, collection_name: str = DEFAULT_COLLECTION_NAME,
             metadata_file: str = "corpus_mapping.pkl") -> bool:
        """
        Charge un index existant depuis le disque
        
        Args:
            collection_name: Nom de la collection
            metadata_file: Fichier du corpus mapping
            
        Returns:
            bool: True si succ√®s
        """
        try:
            logger.info("[LOAD] Chargement index ChromaDB...")
            
            # Initialiser client (utiliser le registre pour √©viter les instances multiples)
            settings = Settings(
                anonymized_telemetry=False,
                is_persistent=True
            )
            self.client = _get_chroma_client(str(self.persist_directory), settings)
            
            # Charger collection
            self.collection = self.client.get_collection(collection_name)
            self.document_count = self.collection.count()
            
            # Charger corpus mapping
            mapping_path = self.persist_directory / metadata_file
            if mapping_path.exists():
                with open(mapping_path, 'rb') as f:
                    self.corpus_mapping = pickle.load(f)
                logger.info(f"[SUCCESS] Corpus mapping charg√©: {len(self.corpus_mapping)} documents")
            else:
                logger.warning("[WARNING] Fichier corpus mapping introuvable, mapping vide")
            
            # R√©cup√©rer dimension depuis m√©tadonn√©es (avec fallback robuste)
            metadata = self.collection.metadata or {}
            dim_value = metadata.get('embedding_dimension')
            if dim_value is not None:
                try:
                    self.embedding_dimension = int(dim_value)
                except Exception:
                    self.embedding_dimension = None
            
            if self.embedding_dimension is None:
                model_meta = (metadata.get('model') or '').lower()
                if 'minilm' in model_meta or '384' in model_meta:
                    self.embedding_dimension = 384
                elif 'labse' in model_meta or '768' in model_meta:
                    self.embedding_dimension = 768
                else:
                    # Par d√©faut raisonnable (MiniLM 384D)
                    self.embedding_dimension = 384
            
            logger.info(f"[SUCCESS] Index charg√©: {self.document_count} documents, dim={self.embedding_dimension}")
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur chargement: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """Statistiques sur le vector store"""
        if not self.collection:
            return {"error": "Index non charg√©"}
        
        return {
            "collection_name": self.collection.name,
            "total_documents": self.document_count,
            "embedding_dimension": self.embedding_dimension,
            "metadata": self.collection.metadata,
            "corpus_mapping_size": len(self.corpus_mapping)
        }


# ============================================================================
# IMPL√âMENTATION 2 : FAISS (Recommand√© pour production)
# ============================================================================

class FAISSVectorStore(BaseVectorStore):
    """
    Vector Store bas√© sur FAISS - Performance Optimale
    
    Avantages vs ChromaDB:
    - Plus rapide (2-3x sur recherches)
    - Moins de RAM
    - Pas de d√©pendances lourdes
    - Fichiers plus petits
    
    Inconv√©nients:
    - Pas de filtrage m√©tadonn√©es natif
    - Plus bas niveau (mais on l'abstrait ici)
    
    Usage:
        >>> store = FAISSVectorStore("./data/faiss_db")
        >>> store.create_index(corpus, embeddings)
        >>> results = store.search(query_embedding, k=5)
    """
    
    def __init__(self, persist_directory: str = "./data/faiss_db"):
        super().__init__(persist_directory)
        self.index = None
        self.corpus_data = []
        self.id_to_idx = {}  # Mapping ID document -> index FAISS
    
    def create_index(self, corpus: List[Dict], embeddings: Union[List, np.ndarray],
                    index_type: str = "Flat") -> bool:
        """
        Cr√©e l'index FAISS
        
        Args:
            corpus: Liste de documents
            embeddings: Embeddings correspondants
            index_type: Type d'index FAISS
                - "Flat" : Exact search (recommand√© <100k docs)
                - "IVF" : Approximate search (>100k docs)
                - "HNSW" : Hierarchical NSW (bon compromis)
        
        Returns:
            bool: True si succ√®s
        """
        try:
            logger.info("[LAUNCH] Cr√©ation index FAISS...")
            start_time = time.time()
            
            # Validation
            self.validate_corpus_data(corpus, embeddings)
            
            # Convertir embeddings en numpy si n√©cessaire
            if not isinstance(embeddings, np.ndarray):
                embeddings = np.array(embeddings, dtype='float32')
            else:
                embeddings = embeddings.astype('float32')
            
            # Normaliser embeddings (pour similarit√© cosinus)
            faiss.normalize_L2(embeddings)
            
            # Cr√©er index selon type
            if index_type == "Flat":
                # Index exact (Inner Product = cosine similarity apr√®s normalisation)
                self.index = faiss.IndexFlatIP(self.embedding_dimension)
                
            elif index_type == "IVF":
                # Index approximatif (plus rapide pour gros corpus)
                nlist = min(100, len(corpus) // 10)  # Nombre de clusters
                quantizer = faiss.IndexFlatIP(self.embedding_dimension)
                self.index = faiss.IndexIVFFlat(quantizer, self.embedding_dimension, nlist)
                self.index.train(embeddings)
                
            elif index_type == "HNSW":
                # Hierarchical Navigable Small World (bon compromis)
                self.index = faiss.IndexHNSWFlat(self.embedding_dimension, 32)
                self.index.hnsw.efConstruction = 40
                
            else:
                raise ValueError(f"Type d'index inconnu: {index_type}")
            
            # Ajouter vecteurs √† l'index
            self.index.add(embeddings)
            
            # Stocker corpus et cr√©er mapping
            self.corpus_data = corpus
            self.id_to_idx = {doc['id']: i for i, doc in enumerate(corpus)}
            self.document_count = len(corpus)
            
            elapsed = time.time() - start_time
            
            logger.info(f"[SUCCESS] Index FAISS cr√©√©: {self.document_count} documents en {elapsed:.2f}s")
            logger.info(f"[STATS] Type: {index_type}, Dimension: {self.embedding_dimension}")
            # √âviter division par z√©ro si elapsed est trop petit
            if elapsed > 1e-6:  # Plus de 1 microseconde
                logger.info(f"[STATS] Vitesse: {self.document_count/elapsed:.1f} docs/sec")
            else:
                logger.info(f"[STATS] Vitesse: > {self.document_count/1e-6:.1f} docs/sec (temps < 1¬µs)")
            
            # Sauvegarder
            self.save()
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur cr√©ation index FAISS: {e}")
            raise
    
    def search(self, query_embedding: Union[List[float], np.ndarray], 
               k: int = 5,
               threshold: float = 0.0) -> List[SearchResult]:
        """
        Recherche s√©mantique avec FAISS
        
        Args:
            query_embedding: Embedding de la question
            k: Nombre de r√©sultats
            threshold: Seuil minimum de similarit√© (0-1)
            
        Returns:
            List[SearchResult]: R√©sultats ordonn√©s
        """
        try:
            if self.index is None:
                raise RuntimeError("Index non initialis√©. Appeler create_index() d'abord.")
            
            # Convertir query en numpy
            if not isinstance(query_embedding, np.ndarray):
                query_embedding = np.array(query_embedding, dtype='float32')
            else:
                query_embedding = query_embedding.astype('float32')
            
            # Reshape si n√©cessaire
            if query_embedding.ndim == 1:
                query_embedding = query_embedding.reshape(1, -1)
            
            # Normaliser pour similarit√© cosinus
            faiss.normalize_L2(query_embedding)
            
            # Recherche
            start_time = time.time()
            similarities, indices = self.index.search(query_embedding, k)
            elapsed = time.time() - start_time
            
            # Formater r√©sultats
            results = []
            
            for rank, (idx, similarity) in enumerate(zip(indices[0], similarities[0]), 1):
                # FAISS retourne -1 si moins de k r√©sultats
                if idx == -1:
                    break
                
                # Appliquer seuil
                if similarity < threshold:
                    continue
                
                doc = self.corpus_data[idx]
                
                results.append(SearchResult(
                    document_id=doc['id'],
                    document_text=doc['contenu'],
                    metadata={
                        'titre': doc.get('titre', 'Unknown'),
                        'source': doc.get('source', 'Unknown'),
                        'organisme': doc.get('organisme', 'Unknown'),
                        'type': doc.get('type', 'general')
                    },
                    similarity_score=float(similarity),
                    rank=rank
                ))
            
            logger.info(f"[SEARCH] Recherche FAISS: {len(results)} r√©sultats en {elapsed*1000:.1f}ms")
            
            return results
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur recherche FAISS: {e}")
            return []
    
    def save(self, index_file: str = "faiss_index.index",
             corpus_file: str = "corpus_data.pkl") -> bool:
        """
        Sauvegarde l'index FAISS et le corpus
        
        Args:
            index_file: Nom fichier index FAISS
            corpus_file: Nom fichier corpus
            
        Returns:
            bool: True si succ√®s
        """
        try:
            # Sauvegarder index FAISS
            index_path = self.persist_directory / index_file
            faiss.write_index(self.index, str(index_path))
            
            # Sauvegarder corpus et m√©tadonn√©es
            metadata = {
                'corpus_data': self.corpus_data,
                'id_to_idx': self.id_to_idx,
                'embedding_dimension': self.embedding_dimension,
                'document_count': self.document_count
            }
            
            corpus_path = self.persist_directory / corpus_file
            with open(corpus_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            logger.info(f" Index FAISS sauvegard√©:")
            logger.info(f"   - Index: {index_path}")
            logger.info(f"   - Corpus: {corpus_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur sauvegarde FAISS: {e}")
            return False
    
    def load(self, index_file: str = "faiss_index.index",
             corpus_file: str = "corpus_data.pkl") -> bool:
        """
        Charge un index FAISS existant
        
        Args:
            index_file: Nom fichier index
            corpus_file: Nom fichier corpus
            
        Returns:
            bool: True si succ√®s
        """
        try:
            logger.info("[LOAD] Chargement index FAISS...")
            
            # Charger index
            index_path = self.persist_directory / index_file
            if not index_path.exists():
                raise FileNotFoundError(f"Index introuvable: {index_path}")
            
            self.index = faiss.read_index(str(index_path))
            
            # Charger corpus
            corpus_path = self.persist_directory / corpus_file
            if not corpus_path.exists():
                raise FileNotFoundError(f"Corpus introuvable: {corpus_path}")
            
            with open(corpus_path, 'rb') as f:
                metadata = pickle.load(f)
            
            self.corpus_data = metadata['corpus_data']
            self.id_to_idx = metadata['id_to_idx']
            self.embedding_dimension = metadata['embedding_dimension']
            self.document_count = metadata['document_count']
            
            logger.info(f"[SUCCESS] Index FAISS charg√©:")
            logger.info(f"   - Documents: {self.document_count}")
            logger.info(f"   - Dimension: {self.embedding_dimension}")
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Erreur chargement FAISS: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """Statistiques sur l'index FAISS"""
        if self.index is None:
            return {"error": "Index non charg√©"}
        
        return {
            "index_type": self.index.__class__.__name__,
            "total_documents": self.document_count,
            "embedding_dimension": self.embedding_dimension,
            "index_size": self.index.ntotal,
            "is_trained": self.index.is_trained if hasattr(self.index, 'is_trained') else True
        }


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def load_corpus_and_embeddings(corpus_path: str = "data/corpus.json",
                               embeddings_path: str = "data/embeddings.npy") -> Tuple[List[Dict], np.ndarray]:
    """
    Charge le corpus et les embeddings depuis les fichiers
    
    Args:
        corpus_path: Chemin vers corpus.json
        embeddings_path: Chemin vers embeddings.npy
        
    Returns:
        Tuple[List[Dict], np.ndarray]: (corpus, embeddings)
    """
    logger.info(f"[LOAD] Chargement corpus: {corpus_path}")
    with open(corpus_path, 'r', encoding='utf-8') as f:
        corpus = json.load(f)
    
    logger.info(f"[LOAD] Chargement embeddings: {embeddings_path}")
    embeddings = np.load(embeddings_path)
    
    logger.info(f"[SUCCESS] Charg√©: {len(corpus)} documents, embeddings shape={embeddings.shape}")
    
    return corpus, embeddings


def compare_vector_stores(corpus: List[Dict], embeddings: np.ndarray, 
                         query_embedding: np.ndarray, k: int = 5):
    """
    Compare les performances ChromaDB vs FAISS
    
    Args:
        corpus: Documents
        embeddings: Embeddings
        query_embedding: Query de test
        k: Nombre de r√©sultats
    """
    logger.info("\n" + "="*60)
    logger.info(" BENCHMARK: ChromaDB vs FAISS")
    logger.info("="*60)
    
    results = {}
    
    # Test ChromaDB
    logger.info("\n[STATS] Test ChromaDB...")
    chroma_store = ChromaVectorStore("./data/benchmark_chroma")
    
    start = time.time()
    chroma_store.create_index(corpus, embeddings, reset=True)
    chroma_index_time = time.time() - start
    
    start = time.time()
    chroma_results = chroma_store.search(query_embedding, k=k)
    chroma_search_time = time.time() - start
    
    results['chroma'] = {
        'index_time': chroma_index_time,
        'search_time': chroma_search_time,
        'results_count': len(chroma_results)
    }
    
    # Test FAISS
    logger.info("\n[STATS] Test FAISS...")
    faiss_store = FAISSVectorStore("./data/benchmark_faiss")
    
    start = time.time()
    faiss_store.create_index(corpus, embeddings)
    faiss_index_time = time.time() - start
    
    start = time.time()
    faiss_results = faiss_store.search(query_embedding, k=k)
    faiss_search_time = time.time() - start
    
    results['faiss'] = {
        'index_time': faiss_index_time,
        'search_time': faiss_search_time,
        'results_count': len(faiss_results)
    }
    
    # Afficher comparaison
    logger.info("\n" + "="*60)
    logger.info("[STATS] R√âSULTATS BENCHMARK")
    logger.info("="*60)
    logger.info(f"\nCHROMADB:")
    logger.info(f"  Temps indexation: {chroma_index_time:.3f}s")
    logger.info(f"  Temps recherche:  {chroma_search_time*1000:.1f}ms")
    
    logger.info(f"\nFAISS:")
    logger.info(f"  Temps indexation: {faiss_index_time:.3f}s")
    logger.info(f"  Temps recherche:  {faiss_search_time*1000:.1f}ms")
    
    logger.info(f"\n GAGNANT:")
    # √âviter division par z√©ro avec une valeur minimale (epsilon)
    epsilon = 1e-6  # 1 microseconde minimum
    safe_chroma_time = max(chroma_search_time, epsilon)
    safe_faiss_time = max(faiss_search_time, epsilon)
    
    if faiss_search_time < chroma_search_time:
        speedup = safe_chroma_time / safe_faiss_time
        logger.info(f"  FAISS est {speedup:.1f}x plus rapide en recherche")
    elif chroma_search_time < faiss_search_time:
        speedup = safe_faiss_time / safe_chroma_time
        logger.info(f"  ChromaDB est {speedup:.1f}x plus rapide en recherche")
    else:
        logger.info(f"  Temps de recherche √©quivalents (diff√©rence < {epsilon*1000:.3f}ms)")
    
    return results


# ============================================================================
# TESTS AUTOMATIS√âS
# ============================================================================

def test_vector_store_complete():
    """
    Suite de tests compl√®te pour les vector stores
    
    Tests:
    1. Cr√©ation index avec donn√©es r√©elles
    2. Recherche s√©mantique
    3. Persistance (save/load)
    4. Performance
    5. Validation dimensions multiples
    """
    logger.info("\n" + "="*70)
    logger.info("[TEST] SUITE DE TESTS COMPL√àTE - VECTOR STORES")
    logger.info("="*70)
    
    try:
        # ===== TEST 1: Donn√©es de test =====
        logger.info("\n Pr√©paration donn√©es de test...")
        
        test_corpus = [
            {
                "id": "doc_001",
                "titre": "Culture du sorgho au Burkina Faso",
                "contenu": "Le sorgho est une c√©r√©ale tr√®s r√©sistante √† la s√©cheresse, particuli√®rement adapt√©e au climat sah√©lien du Burkina Faso. Il n√©cessite 400-600mm d'eau par saison. L'engrais NPK 14-23-14 est recommand√© √† raison de 150-200 kg/ha.",
                "source": "FAO - Guide technique 2023",
                "organisme": "FAO",
                "type": "guide_technique"
            },
            {
                "id": "doc_002",
                "titre": "Fertilisation du mil",
                "contenu": "Le mil n√©cessite une fertilisation azot√©e pour de bons rendements. Application d'ur√©e √† 30-40 jours apr√®s semis am√©liore significativement la production. Le mil tol√®re mieux la s√©cheresse que le ma√Øs.",
                "source": "CIRAD - Fiche pratique",
                "organisme": "CIRAD",
                "type": "fiche_technique"
            },
            {
                "id": "doc_003",
                "titre": "Mara√Æchage urbain √† Ouagadougou",
                "contenu": "Le mara√Æchage urbain conna√Æt un essor important dans la capitale. Les cultures principales sont la tomate, l'oignon, le chou. L'irrigation goutte-√†-goutte permet d'√©conomiser l'eau. Fumure organique recommand√©e.",
                "source": "INSD - Enqu√™te agricole 2024",
                "organisme": "INSD",
                "type": "statistiques"
            },
            {
                "id": "doc_004",
                "titre": "Protection contre les ravageurs",
                "contenu": "Les insectes ravageurs causent des pertes importantes. Lutte int√©gr√©e recommand√©e: rotation des cultures, vari√©t√©s r√©sistantes, traitement au neem. √âviter usage excessif de pesticides chimiques.",
                "source": "GIZ - Programme agriculture durable",
                "organisme": "GIZ",
                "type": "guide_pratique"
            },
            {
                "id": "doc_005",
                "titre": "Calendrier cultural r√©gion Centre",
                "contenu": "R√©gion Centre (Ouagadougou): Plantation sorgho et mil en juin-juillet. Ma√Øs peut √™tre plant√© d√®s mai si pluies pr√©coces. R√©colte octobre-novembre. P√©riode critique pour eau: montaison et floraison.",
                "source": "Minist√®re Agriculture BF",
                "organisme": "MINAGRI",
                "type": "calendrier"
            }
        ]
        
        # G√©n√©rer embeddings de test (simulation LaBSE 768D)
        np.random.seed(42)
        test_embeddings = np.random.randn(5, 768).astype('float32')
        
        # Normaliser pour similarit√© r√©aliste
        test_embeddings = test_embeddings / np.linalg.norm(test_embeddings, axis=1, keepdims=True)
        
        logger.info(f"[SUCCESS] {len(test_corpus)} documents de test cr√©√©s")
        logger.info(f"[SUCCESS] Embeddings shape: {test_embeddings.shape}")
        
        # ===== TEST 2: ChromaDB =====
        logger.info("\n" + "-"*70)
        logger.info(" TEST CHROMADB")
        logger.info("-"*70)
        
        chroma_store = ChromaVectorStore("./data/test_chroma")
        
        # Test cr√©ation index
        logger.info("\n1Ô∏è‚É£ Test cr√©ation index...")
        success = chroma_store.create_index(test_corpus, test_embeddings, reset=True)
        assert success, "[ERROR] √âchec cr√©ation index ChromaDB"
        logger.info("[SUCCESS] Index ChromaDB cr√©√©")
        
        # Test recherche
        logger.info("\n2Ô∏è‚É£ Test recherche...")
        query_embedding = test_embeddings[0]  # Utiliser premier doc comme query
        results = chroma_store.search(query_embedding, k=3)
        
        assert len(results) > 0, "[ERROR] Aucun r√©sultat de recherche"
        logger.info(f"[SUCCESS] {len(results)} r√©sultats trouv√©s")
        
        for i, result in enumerate(results, 1):
            logger.info(f"\n   R√©sultat #{i}:")
            logger.info(f"   - ID: {result.document_id}")
            logger.info(f"   - Titre: {result.metadata['titre']}")
            logger.info(f"   - Score: {result.similarity_score:.4f}")
            logger.info(f"   - Extrait: {result.document_text[:100]}...")
        
        # Test persistance
        logger.info("\n3Ô∏è‚É£ Test save/load...")
        save_success = chroma_store.save()
        assert save_success, "[ERROR] √âchec sauvegarde ChromaDB"
        
        # Cr√©er nouvelle instance et charger
        chroma_store2 = ChromaVectorStore("./data/test_chroma")
        load_success = chroma_store2.load()
        assert load_success, "[ERROR] √âchec chargement ChromaDB"
        
        # V√©rifier recherche apr√®s reload
        results2 = chroma_store2.search(query_embedding, k=3)
        assert len(results2) == len(results), "[ERROR] R√©sultats diff√©rents apr√®s reload"
        logger.info("[SUCCESS] Persistance ChromaDB valid√©e")
        
        # Statistiques
        logger.info("\n4Ô∏è‚É£ Statistiques ChromaDB:")
        stats = chroma_store.get_statistics()
        for key, value in stats.items():
            logger.info(f"   - {key}: {value}")
        
        # ===== TEST 3: FAISS =====
        logger.info("\n" + "-"*70)
        logger.info("üü¢ TEST FAISS")
        logger.info("-"*70)
        
        faiss_store = FAISSVectorStore("./data/test_faiss")
        
        # Test cr√©ation index
        logger.info("\n1Ô∏è‚É£ Test cr√©ation index FAISS...")
        success = faiss_store.create_index(test_corpus, test_embeddings)
        assert success, "[ERROR] √âchec cr√©ation index FAISS"
        logger.info("[SUCCESS] Index FAISS cr√©√©")
        
        # Test recherche
        logger.info("\n2Ô∏è‚É£ Test recherche FAISS...")
        results = faiss_store.search(query_embedding, k=3)
        
        assert len(results) > 0, "[ERROR] Aucun r√©sultat FAISS"
        logger.info(f"[SUCCESS] {len(results)} r√©sultats trouv√©s")
        
        for i, result in enumerate(results, 1):
            logger.info(f"\n   R√©sultat #{i}:")
            logger.info(f"   - ID: {result.document_id}")
            logger.info(f"   - Titre: {result.metadata['titre']}")
            logger.info(f"   - Score: {result.similarity_score:.4f}")
        
        # Test persistance
        logger.info("\n3Ô∏è‚É£ Test save/load FAISS...")
        save_success = faiss_store.save()
        assert save_success, "[ERROR] √âchec sauvegarde FAISS"
        
        # Reload
        faiss_store2 = FAISSVectorStore("./data/test_faiss")
        load_success = faiss_store2.load()
        assert load_success, "[ERROR] √âchec chargement FAISS"
        
        results2 = faiss_store2.search(query_embedding, k=3)
        assert len(results2) == len(results), "[ERROR] R√©sultats diff√©rents apr√®s reload"
        logger.info("[SUCCESS] Persistance FAISS valid√©e")
        
        # Statistiques
        logger.info("\n4Ô∏è‚É£ Statistiques FAISS:")
        stats = faiss_store.get_statistics()
        for key, value in stats.items():
            logger.info(f"   - {key}: {value}")
        
        # ===== TEST 4: Benchmark =====
        logger.info("\n" + "-"*70)
        logger.info("‚ö° BENCHMARK PERFORMANCE")
        logger.info("-"*70)
        
        benchmark_results = compare_vector_stores(test_corpus, test_embeddings, query_embedding, k=3)
        
        # ===== TEST 5: Validation dimensions =====
        logger.info("\n" + "-"*70)
        logger.info("[FIX] TEST DIMENSIONS MULTIPLES")
        logger.info("-"*70)
        
        # Test avec dimension 384 (MiniLM)
        logger.info("\nTest dimension 384 (MiniLM)...")
        embeddings_384 = np.random.randn(5, 384).astype('float32')
        embeddings_384 = embeddings_384 / np.linalg.norm(embeddings_384, axis=1, keepdims=True)
        
        faiss_store_384 = FAISSVectorStore("./data/test_faiss_384")
        success = faiss_store_384.create_index(test_corpus, embeddings_384)
        assert success, "[ERROR] √âchec dimension 384"
        logger.info("[SUCCESS] Dimension 384 support√©e")
        
        # ===== R√âSUM√â =====
        logger.info("\n" + "="*70)
        logger.info("[CELEBRATE] TOUS LES TESTS R√âUSSIS")
        logger.info("="*70)
        logger.info("\n[SUCCESS] ChromaDB: OK")
        logger.info("[SUCCESS] FAISS: OK")
        logger.info("[SUCCESS] Persistance: OK")
        logger.info("[SUCCESS] Recherche s√©mantique: OK")
        logger.info("[SUCCESS] Dimensions multiples: OK")
        logger.info("[SUCCESS] Performance: OK")
        
        return True
        
    except AssertionError as e:
        logger.error(f"\n[ERROR] TEST √âCHOU√â: {e}")
        return False
    except Exception as e:
        logger.error(f"\n[ERROR] ERREUR INATTENDUE: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# SCRIPT PRINCIPAL - INT√âGRATION AVEC VOTRE PIPELINE
# ============================================================================

def main():
    """
    Script principal - Utilisation avec votre corpus existant
    
    Sc√©narios:
    1. Premi√®re fois: Cr√©er index depuis corpus.json + embeddings.npy
    2. Utilisation: Charger index existant
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Vector Store pour RAG Agricole BF')
    parser.add_argument('--mode', choices=['create', 'load', 'test', 'benchmark'],
                       default='test', help='Mode op√©ration')
    parser.add_argument('--backend', choices=['chroma', 'faiss'], 
                       default='faiss', help='Backend vector store')
    parser.add_argument('--corpus', default='data/corpus.json',
                       help='Chemin corpus.json')
    parser.add_argument('--embeddings', default='data/embeddings.npy',
                       help='Chemin embeddings.npy')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        # Lancer tests automatis√©s
        logger.info("[TEST] Mode TEST - Suite de tests compl√®te")
        test_vector_store_complete()
        
    elif args.mode == 'benchmark':
        # Benchmark ChromaDB vs FAISS
        logger.info("‚ö° Mode BENCHMARK")
        corpus, embeddings = load_corpus_and_embeddings(args.corpus, args.embeddings)
        query_emb = embeddings[0]  # Premi√®re doc comme query
        compare_vector_stores(corpus, embeddings, query_emb, k=5)
        
    elif args.mode == 'create':
        # Cr√©er nouvel index
        logger.info(f" Mode CREATE - Backend: {args.backend}")
        
        # Charger donn√©es
        corpus, embeddings = load_corpus_and_embeddings(args.corpus, args.embeddings)
        
        # Cr√©er index selon backend
        if args.backend == 'chroma':
            store = ChromaVectorStore("./data/chroma_db")
            store.create_index(corpus, embeddings, reset=True)
        else:
            store = FAISSVectorStore("./data/faiss_db")
            store.create_index(corpus, embeddings)
        
        logger.info("[SUCCESS] Index cr√©√© avec succ√®s")
        
        # Test rapide
        logger.info("\n[TEST] Test rapide...")
        results = store.search(embeddings[0], k=3)
        logger.info(f"[SUCCESS] Recherche OK: {len(results)} r√©sultats")
        
    elif args.mode == 'load':
        # Charger index existant
        logger.info(f"[LOAD] Mode LOAD - Backend: {args.backend}")
        
        if args.backend == 'chroma':
            store = ChromaVectorStore("./data/chroma_db")
            success = store.load()
        else:
            store = FAISSVectorStore("./data/faiss_db")
            success = store.load()
        
        if success:
            logger.info("[SUCCESS] Index charg√© avec succ√®s")
            stats = store.get_statistics()
            logger.info("\n[STATS] Statistiques:")
            for key, value in stats.items():
                logger.info(f"   - {key}: {value}")
        else:
            logger.error("[ERROR] √âchec chargement index")


# ============================================================================
# EXEMPLE D'UTILISATION DANS VOTRE PIPELINE RAG
# ============================================================================

class ExempleIntegrationRAG:
    """
    Exemple d'int√©gration du VectorStore dans votre pipeline RAG
    """
    
    def __init__(self, use_faiss: bool = True):
        """
        Args:
            use_faiss: Si True, utilise FAISS (plus rapide), sinon ChromaDB
        """
        if use_faiss:
            self.vector_store = FAISSVectorStore("./data/faiss_db")
            logger.info("üü¢ VectorStore: FAISS")
        else:
            self.vector_store = ChromaVectorStore("./data/chroma_db")
            logger.info(" VectorStore: ChromaDB")
        
        self.initialized = False
    
    def setup(self, corpus_path: str = "data/corpus.json",
              embeddings_path: str = "data/embeddings.npy"):
        """
        Configuration initiale: cr√©er index
        √Ä faire UNE SEULE FOIS
        """
        logger.info("[LAUNCH] Setup VectorStore...")
        
        # Charger donn√©es
        corpus, embeddings = load_corpus_and_embeddings(corpus_path, embeddings_path)
        
        # Cr√©er index
        self.vector_store.create_index(corpus, embeddings)
        self.initialized = True
        
        logger.info("[SUCCESS] VectorStore pr√™t")
    
    def load_existing(self):
        """
        Charger index existant
        √Ä faire √† chaque d√©marrage de l'application
        """
        logger.info("[LOAD] Chargement VectorStore existant...")
        
        success = self.vector_store.load()
        if success:
            self.initialized = True
            logger.info("[SUCCESS] VectorStore charg√©")
        else:
            logger.error("[ERROR] √âchec chargement - Lancer setup() d'abord")
        
        return success
    
    def retrieve_documents(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:
        """
        R√©cup√®re documents pertinents pour une question
        
        Args:
            query_embedding: Embedding de la question
            k: Nombre de documents √† r√©cup√©rer
            
        Returns:
            List[Dict]: Documents pertinents avec m√©tadonn√©es
        """
        if not self.initialized:
            raise RuntimeError("VectorStore non initialis√©. Appeler setup() ou load_existing()")
        
        # Recherche
        results = self.vector_store.search(query_embedding, k=k)
        
        # Formater pour le LLM
        formatted_docs = []
        for result in results:
            formatted_docs.append({
                'id': result.document_id,
                'text': result.document_text,
                'metadata': result.metadata,
                'score': result.similarity_score,
                'rank': result.rank
            })
        
        return formatted_docs


if __name__ == "__main__":
    # Si lanc√© directement, ex√©cuter le script principal
    main()